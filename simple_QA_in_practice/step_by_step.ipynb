{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from ...\n",
      "Loaded: 50004 words, 4336 training samples\n",
      "Shuffling dataset...\n"
     ]
    }
   ],
   "source": [
    "from text_data_2 import *\n",
    "from create_dict_2 import *\n",
    "import model_seq2seq_2\n",
    "from model_seq2seq_2 import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.app.flags.DEFINE_float('learning_rate',0.5,'Learning rate')\n",
    "tf.app.flags.DEFINE_float('learning_rate_decay_factor',0.99,'Learning rate decays by this much')\n",
    "tf.app.flags.DEFINE_float('max_gradient_norm',5.0,'Clip gradients to this norm')\n",
    "tf.app.flags.DEFINE_integer('batch_size',64,'Batch size for training')\n",
    "tf.app.flags.DEFINE_integer('size',20,'Size of each layer') # the number of unfolded LSTM units in each layer(maybe too larger)..? or the dimension of hidden vector?\n",
    "# the number of unfolded LSTM units could have big influence on the time need to build the model, 512 units require a long time\n",
    "tf.app.flags.DEFINE_integer('num_layers',3,'Number of layers')\n",
    "tf.app.flags.DEFINE_integer('num_epochs',10,'maximum number of epochs to run')\n",
    "tf.app.flags.DEFINE_integer('vocab_size',50000,'Vocabulary size, words with lower frequency are regarded as unknown')\n",
    "tf.app.flags.DEFINE_string('train_samples_path','ubuntu_train_samples.pkl','Processed training samples')\n",
    "tf.app.flags.DEFINE_string('valid_samples_path','ubuntu_valid_samples.pkl','Processed validation samples')\n",
    "# tf.app.flags.DEFINE_string('test_samples_path','ubuntu_test_samples.pkl','Processed test samples')\n",
    "rootDir = \"/Users/hanzhichao/Documents/ETH_Courses/DeepLearning/project/\"\n",
    "tf.app.flags.DEFINE_string('train_data_path',rootDir+'data/ubuntu/train.csv','Training data')\n",
    "tf.app.flags.DEFINE_string('valid_data_path',rootDir+'data/ubuntu/valid.csv','Validation data')\n",
    "# tf.app.flags.DEFINE_string('test_data_path','data/ubuntu/test.csv','Test data')\n",
    "tf.app.flags.DEFINE_string('sorted_list_path','ubuntu_freqlist.pkl','List of words sorted by frequency')\n",
    "tf.app.flags.DEFINE_string('dialog_path',rootDir+'data/ubuntu/dialogs/','Directory of raw ubuntu dialogues')\n",
    "tf.app.flags.DEFINE_integer('playDataset',20,'Display random samples from data')\n",
    "tf.app.flags.DEFINE_integer('max_train_data_size',0,'Limit on the size of training data (0: no limit)')\n",
    "tf.app.flags.DEFINE_integer('steps_per_checkpoint',500,'Number of training steps between checkpoints')\n",
    "tf.app.flags.DEFINE_boolean('decode',False,'Set to True for interactive decoding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_buckets = [(10,20),(20,40),(30,60),(40,80),(50,100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(session):\n",
    "    model = Model(FLAGS)\n",
    "    # TODO: save and restore model to/from hard disk\n",
    "    Load_model = False\n",
    "    if Load_model:\n",
    "        model.saver.restore(session, FILA_NAME)\n",
    "    else:\n",
    "        session.run(tf.initialize_all_variables())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(model_seq2seq_2)\n",
    "from model_seq2seq_2 import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model creation...\n",
      "3\n",
      "starting building network...\n",
      "WARNING:tensorflow:From model_seq2seq_2.py:76 in buildNetwork.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "network built...\n"
     ]
    }
   ],
   "source": [
    "model = Model(FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'placeholder_encoder/Placeholder:0' shape=(?,) dtype=int32>,\n",
       " <tf.Tensor 'placeholder_encoder/Placeholder_1:0' shape=(?,) dtype=int32>,\n",
       " <tf.Tensor 'placeholder_encoder/Placeholder_2:0' shape=(?,) dtype=int32>,\n",
       " <tf.Tensor 'placeholder_encoder/Placeholder_3:0' shape=(?,) dtype=int32>,\n",
       " <tf.Tensor 'placeholder_encoder/Placeholder_4:0' shape=(?,) dtype=int32>,\n",
       " <tf.Tensor 'placeholder_encoder/Placeholder_5:0' shape=(?,) dtype=int32>,\n",
       " <tf.Tensor 'placeholder_encoder/Placeholder_6:0' shape=(?,) dtype=int32>,\n",
       " <tf.Tensor 'placeholder_encoder/Placeholder_7:0' shape=(?,) dtype=int32>,\n",
       " <tf.Tensor 'placeholder_encoder/Placeholder_8:0' shape=(?,) dtype=int32>,\n",
       " <tf.Tensor 'placeholder_encoder/Placeholder_9:0' shape=(?,) dtype=int32>,\n",
       " <tf.Tensor 'placeholder_encoder/Placeholder_10:0' shape=(?,) dtype=int32>,\n",
       " <tf.Tensor 'placeholder_encoder/Placeholder_11:0' shape=(?,) dtype=int32>,\n",
       " <tf.Tensor 'placeholder_encoder/Placeholder_12:0' shape=(?,) dtype=int32>,\n",
       " <tf.Tensor 'placeholder_encoder/Placeholder_13:0' shape=(?,) dtype=int32>,\n",
       " <tf.Tensor 'placeholder_encoder/Placeholder_14:0' shape=(?,) dtype=int32>,\n",
       " <tf.Tensor 'placeholder_encoder/Placeholder_15:0' shape=(?,) dtype=int32>,\n",
       " <tf.Tensor 'placeholder_encoder/Placeholder_16:0' shape=(?,) dtype=int32>,\n",
       " <tf.Tensor 'placeholder_encoder/Placeholder_17:0' shape=(?,) dtype=int32>,\n",
       " <tf.Tensor 'placeholder_encoder/Placeholder_18:0' shape=(?,) dtype=int32>,\n",
       " <tf.Tensor 'placeholder_encoder/Placeholder_19:0' shape=(?,) dtype=int32>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoderInputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-9f5df2c7784a>:1 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ubuntu_train_samples.pkl'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FLAGS.train_samples_path\n",
    "# train_set = TextData(FLAGS.train_samples_path, FLAGS.train_data_path, FLAGS.sorted_list_path, FLAGS.dialog_path, FLAGS.vocab_size, FLAGS.playDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/hanzhichao/Documents/ETH_Courses/DeepLearning/project/data/ubuntu/dialogs/'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FLAGS.dialog_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ubuntu_freqlist.pkl'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FLAGS.sorted_list_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from ...\n",
      "Loaded: 50004 words, 4336 training samples\n",
      "Randomly play samples:\n",
      "Context: greetings i am just exploring cronjobs there is a php script that i want to run daily it 's spelled `` perl '' not `` php '' bad joke . go on ...\n",
      "Utterance: oh okay : p. so i was wondering , would the right way be to set up a cronjob that calls curl to access my php script ? <eos>\n",
      "Context: if i want <unknown> to resolve to <unknown> on my local box do n't i just add <unknown> <unknown> in my /etc/hosts file ? yes\n",
      "Utterance: i did that and ping it and it does n't beleive me . it wants to use dns instead <eos>\n",
      "Context: god dammit http : <unknown> also , control your language here , please\n",
      "Utterance: sorry <eos>\n",
      "Context: ! sp | <unknown> ! es | <unknown> <unknown> http : <unknown>\n",
      "Utterance: i do n't know why it 's not using it , although it 's activated . you may find more support on # ubuntu-it , try to ask <unknown> or <unknown> <eos>\n",
      "Context: hello ? if i install python3 , my current version of python2 , will be replaced ? no\n",
      "Utterance: thanks <eos>\n",
      "Context: how can i install ubuntu onto a thumbdrive ? pendrivelinux.com\n",
      "Utterance: thanks <eos>\n",
      "Context: oh crap , all my youtube videos have a blue tint . how to fix ? use the html5 feature of youtube and dump flsh how to do that ?\n",
      "Utterance: youtube.com/html5 yep . i noticed the same thing . or just enable html5 for youtube use the console . login , try <unknown> <unknown> .xauthority file . then try logging into with lightdm again <eos>\n",
      "Context: what is the command to remove an old kernel and all the files it comes with ? yes but a kernel is not only one single file , is it . uname -a\n",
      "Utterance: that only shows the one in use <eos>\n",
      "Context: whats the best way to back up my entire ~/ including keys etc i get permission denied errors permission denied for what ? i was just using tar cf <unknown> /home/me\n",
      "Utterance: you must have <unknown> some stuff as root in there run the tar with sudo , or find out which files are not owned by your user and why <eos>\n",
      "Context: how do i delete all files in a folder that doesnt end in .txt extension ? find path/ -type f -not -iname <unknown> ' -delete for rtf in <unknown> ; do <unknown> -- -- <unknown> `` $ file '' ; done what does the `` $ file '' do ?\n",
      "Utterance: sry `` $ rtf '' not $ file <eos>\n",
      "Context: whats up with all these spammers ? has this become the norm on this channel ? it 's a popular channel , it will attract idiots yeah , i <unknown> it was popular before though and i dont remember seeing such types of spam here\n",
      "Utterance: idk , i never see it in other linux channels , leads me to believe its just because ubuntu is popular ( and thus , near the top of the list ) <eos>\n",
      "Context: what are my hopes for fixing an interupted partition resize anyways ? depends where it got interrupted , but generally , not good\n",
      "Utterance: stupid <unknown> <unknown> x-server crashed on me and took gparted with it <eos>\n",
      "Context: ! find aircrack http : <unknown>\n",
      "Utterance: thanks for the link but i do n't need it <eos>\n",
      "Context: this one too please ikonia , this one too no need if you can use the channel correctly\n",
      "Utterance: this one too please <eos>\n",
      "Context: what make/model ? <unknown> e 157 3g stick\n",
      "Utterance: looking ... this might help you , it describes your situation pretty well https : <unknown> <eos>\n",
      "Context: what does ubuntu use to extract files ( throught nautilus ) ? i think it 's called file roller\n",
      "Utterance: thanks nuts ! <eos>\n",
      "Context: what is the appropriate mount point for swap ? swap is among the filesystem choices\n",
      "Utterance: escott just found that out lol.. trying to be <unknown> wtih questions - look stupid : ( <eos>\n",
      "Context: click in the dash and write : nvidia there is no dash <unknown> sudo add-apt-repository ppa : ubuntu-x-swat/x-updates ; sudo apt-get update ; sudo apt-get upgrade\n",
      "Utterance: no ... <eos>\n",
      "Context: do-release-upgrade will tweak your sources.list file for you and check if a cd is inserted <unknown> tweaking ? i 'm not sure about the cd part , it may ... why do you ask ?\n",
      "Utterance: want to fetch the major part of the updates from the official cd server version <eos>\n",
      "Context: anyone runs ubuntu through vmware ? i do\n",
      "Utterance: can ubuntu <unknown> laptops built in wifi adapter so i can track wireless connections in range ? ..when ran through vmware <eos>\n"
     ]
    }
   ],
   "source": [
    "train_set = TextData(FLAGS.train_samples_path, FLAGS.train_data_path, FLAGS.sorted_list_path, FLAGS.dialog_path, FLAGS.vocab_size, FLAGS.playDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from ...\n",
      "Loaded: 50004 words, 4336 training samples\n",
      "Randomly play samples:\n",
      "Context: it 's so awful xd or open another terminal\n",
      "Utterance: yeah , that 's the way . <eos>\n",
      "Context: how do i completely disable 3d effects in 12.04 ? use unity 2d you mean ? or some other desktop .\n",
      "Utterance: unity 2d ? i think i 'm already using unity 2d so i need to select 'ubuntu 2d ' at the login screen ? <eos>\n",
      "Context: i 'll try that , thank you <unknown> : ) btw , how do you get `` my nick '' in front of message ? quite nifty , makes it quite a lot easier to chat in irc ; p type three letters and hit tab\n",
      "Utterance: aha got it ! thanks guys ; ) <eos>\n",
      "Context: is there a way to make 12.04 login faster ? it takes me about 20 seconds from when i enter the password to when i have a working desktop hardware specs ?\n",
      "Utterance: i 've got a hp pavillion <unknown> , i do n't know the specs exactly <eos>\n",
      "Context: anyone know how to kill a failed installation ? kill -9 is n't working kill -9 is the lowest kill you can use , it 's probably zombied it 's not showing as a zombied process .\n",
      "Utterance: which process is it ? <eos>\n",
      "Context: http : <unknown> resolvconf replaces /etc/resolv.conf looking at that now .\n",
      "Utterance: read the page again , it tells you how to override it . is /etc/resolv.conf a symlink ? <eos>\n",
      "Context: menu* oh interesting <unknown> : and by esc , i think kx means hold down shift : )\n",
      "Utterance: shit does the same thing ? <unknown> does the same thing ? <eos>\n",
      "Context: any hints how to decrypt a home filesystem via ecryptfs manually ( i do know the passphrase , i 'm just trying to restore some files from a backup ) cryptsetup luksopen something someting\n",
      "Utterance: encrypted home filesystem = > that 's not <unknown> encryption . <eos>\n",
      "Context: ahoy . anyone gotten dual or <unknown> working in precise with nvidia-current ? i use twin view works well i 'm a noob . does that mean mirrored displays ? no\n",
      "Utterance: right on , sounds like what i need . currently i 've got one monitor working and the other is a gray screen and the display manager control panel applet does n't see both monitors . <eos>\n",
      "Context: we 're actually rather offtopic , talking about trying sabayon ! oh ; - )\n",
      "Utterance: ... which is to say , no , but there 's a webpage . <eos>\n",
      "Context: @ <unknown> we use skype in ubuntu ? ? ? some better than others\n",
      "Utterance: <unknown> : : can u say me installation <unknown> wat r d best options ? <eos>\n",
      "Context: is n't it april ? eh ?\n",
      "Utterance: true sry <eos>\n",
      "Context: could use top installed but yet to run it 's running here but no notification area icon . i wonder how to get that to work .\n",
      "Utterance: are there bugs reported ? <eos>\n",
      "Context: hi i need to read a .bin file ? i need to read a .bin file ? you ca n't read you can run it\n",
      "Utterance: que <unknown> tu dire par la ? <eos>\n",
      "Context: test success , but in future , # test exists\n",
      "Utterance: haha , i was checking if my messages are going through . thanks for confirming . <eos>\n",
      "Context: what device gives you the <unknown> connection ? gfx card\n",
      "Utterance: and no , the sound card is not involved <eos>\n",
      "Context: ! find <unknown> thanks .\n",
      "Utterance: you 're welcome <eos>\n",
      "Context: fat32 ? yes\n",
      "Utterance: could be why . <eos>\n",
      "Context: wii game are you <unknown> with playonlinux ?\n",
      "Utterance: uninstalling playonlinux now and emptying trash in .local/share/trash cause i dont run it and its taking up alot of room <eos>\n",
      "Context: seems rather strange to change an old well proven setup like resolv.conf without checking that it is compatible with other packages in 12.04 they started using a resolvconf daemon which monitors changes to network configuration and makes /etc/resolv.conf entries automatically apparently it isnt monitoring my vpn tunnel correctly ; )\n",
      "Utterance: i 'm sorry , i do n't have experiance configuring it with vpn tunnels . in the case of a static configuration on an interface , you can add <unknown> ' entries into your /etc/network/interfaces file . <eos>\n"
     ]
    }
   ],
   "source": [
    "dev_set = TextData(FLAGS.valid_samples_path, FLAGS.valid_data_path, FLAGS.sorted_list_path, FLAGS.dialog_path, FLAGS.vocab_size, FLAGS.playDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "step_time, loss = 0.0, 0.0\n",
    "current_step = 0\n",
    "previous_loss = []\n",
    "model.learning_rate\n",
    "FLAGS.num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----- Epoch 1/10 ; (lr=0.5) -----\n",
      "Shuffling dataset...\n",
      "\n",
      "\n",
      "----- Epoch 2/10 ; (lr=0.5) -----\n",
      "Shuffling dataset...\n",
      "\n",
      "\n",
      "----- Epoch 3/10 ; (lr=0.5) -----\n",
      "Shuffling dataset...\n",
      "\n",
      "\n",
      "----- Epoch 4/10 ; (lr=0.5) -----\n",
      "Shuffling dataset...\n",
      "\n",
      "\n",
      "----- Epoch 5/10 ; (lr=0.5) -----\n",
      "Shuffling dataset...\n",
      "\n",
      "\n",
      "----- Epoch 6/10 ; (lr=0.5) -----\n",
      "Shuffling dataset...\n",
      "\n",
      "\n",
      "----- Epoch 7/10 ; (lr=0.5) -----\n",
      "Shuffling dataset...\n",
      "\n",
      "\n",
      "----- Epoch 8/10 ; (lr=0.5) -----\n",
      "Shuffling dataset...\n",
      "\n",
      "\n",
      "----- Epoch 9/10 ; (lr=0.5) -----\n",
      "Shuffling dataset...\n",
      "\n",
      "\n",
      "----- Epoch 10/10 ; (lr=0.5) -----\n",
      "Shuffling dataset...\n"
     ]
    }
   ],
   "source": [
    "for e in range(FLAGS.num_epochs):\n",
    "    print (\"\\n\")\n",
    "    print(\"----- Epoch {}/{} ; (lr={}) -----\".format(e+1, FLAGS.num_epochs, model.learning_rate))\n",
    "    batches = train_set.get_batches(FLAGS.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tic = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/68 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Batch instance has no attribute 'encoderSeqs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-e70dcd3bb579>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnextBatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeedDict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnextBatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/hanzhichao/Documents/github/Conversational-Agents/simple_QA_practice/model_seq2seq_2.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxLengthEnco\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                 \u001b[0mfeedDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoderInputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoderSeqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxLengthDeco\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mfeedDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoderInputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoderSeqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Ref: DeepQA/model.py/step(); DeepQA/textdata.py/line 121 and line 122\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Batch instance has no attribute 'encoderSeqs'"
     ]
    }
   ],
   "source": [
    "for nextBatch in tqdm(batches, desc=\"Training\"):\n",
    "    start_time = time.time()\n",
    "    ops, feedDict = model.step(nextBatch) # inferface with train_set.get_batches(FLAGS.batch_size) has some problem..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
